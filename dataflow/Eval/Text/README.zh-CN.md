
# æ–‡æœ¬æ•°æ®è´¨é‡è¯„ä¼°

æœ¬æ•°æ®è¯„ä¼°ç³»ç»Ÿç›®å‰å·²æ•´åˆäº†**20ç§ä¸åŒç±»å‹çš„å‰æ²¿æ–‡æœ¬æ•°æ®è¯„ä¼°æ–¹æ³•**ä»¥åŠåä½™ç§ç”Ÿæˆæ–‡æœ¬è¯„ä¼°æ–¹æ³•ã€‚è¯¦è§[è¯„ä¼°ç®—æ³•æ–‡æ¡£](../../../docs/text_metrics.zh-CN.md)ã€[ç”Ÿæˆæ–‡æœ¬è¯„ä¼°ç®—æ³•æ–‡æ¡£](../../../docs/gen_text_metrics.zh-CN.md)ã€‚åœ¨è¿›è¡Œæ•°æ®è¯„ä¼°æ—¶ï¼Œå¯é€šè¿‡`yaml`é…ç½®æ–‡ä»¶æŒ‡å®šæ•°æ®æºã€æ•°æ®æ ¼å¼ã€æ‰“åˆ†å™¨ä»¥åŠæ‰“åˆ†å™¨é…ç½®ä¿¡æ¯ã€‚ç”¨æˆ·å¯é€šè¿‡æ›´æ”¹é…ç½®æ–‡ä»¶çš„æ–¹å¼å¯¹ä¸åŒçš„æ–‡æœ¬æ•°æ®è¿›è¡Œè¯„ä¼°ã€‚


## ğŸ‘€ é…ç½®æ–‡ä»¶

é…ç½®æ–‡ä»¶å­˜æ”¾åœ¨`DataFlow/configs/eval`ä¸­ï¼Œä¾‹å¦‚

```yaml
model_cache_path: '../ckpt' # æ¨¡å‹é»˜è®¤ç¼“å­˜è·¯å¾„
dependencies: [text] # é€‰æ‹©è¦åŠ è½½çš„ç¯å¢ƒä¾èµ–
save_path: "./scores" # è¾“å‡ºåˆ†æ•°å­˜å‚¨è·¯å¾„

data:
  text:
    use_hf: False # æ˜¯å¦ä½¿ç”¨huggingface_datasetï¼Œå¦‚æœä½¿ç”¨åˆ™å¿½ç•¥ä¸‹æ–¹æœ¬åœ°æ•°æ®åœ°å€ï¼Œä»¥ä¸‹ä¸ºHuggingfaceæ•°æ®é›†å‚æ•°è®¾ç½®ï¼›å¦‚æœä¸ä½¿ç”¨åˆ™å¿½ç•¥ã€‚
    dataset_name: 'MBZUAI-LLM/SlimPajama-627B-DC'
    dataset_split: 'test'
    revision: 'refs/convert/parquet'
    name: 'default'
    data_path: 'demos/text_eval/fineweb_5_samples.json'  # æœ¬åœ°æ•°æ®åœ°å€ï¼Œæ”¯æŒjsonã€jsonlã€parquetæ ¼å¼
    formatter: "TextFormatter" # æ•°æ®åŠ è½½å™¨ç±»å‹ï¼Œä½¿ç”¨TextFormatterå³å¯

    keys: 'text' # å¾…è¯„ä¼°çš„é”®åï¼Œå¯¹äºsftæ•°æ®ï¼Œå¯æŒ‡å®šä¸º['instruction','input','output']
    
scorers: # å¯ä»all_scorers.yamlä¸­é€‰æ‹©å¤šä¸ªtextæ‰“åˆ†å™¨ï¼Œå°†å…¶é…ç½®ä¿¡æ¯æ”¾å…¥å³å¯
  PresidioScorer:
      language: 'en'
      device: 'cuda:0'
  QuratingScorer:
      model: 'princeton-nlp/QuRater-1.3B'
      tokens_field: 'input_ids'
      tokens: 512
      map_batch_size: 512
      num_workers: 1
      device_batch_size: 16
      device: 'cuda:0'
      labels:
        - writing_style
        - required_expertise
        - facts_and_trivia
        - educational_value
```

å¯¹äºç”Ÿæˆæ–‡æœ¬ï¼Œé…ç½®æ–‡ä»¶éœ€è¦æŒ‡å®šå¾…è¯„ä¼°æ•°æ®é›†å’Œå‚è€ƒæ•°æ®é›†æ–‡ä»¶ä»¥åŠé”®åã€‚
```yaml
dependencies: [text] # é€‰æ‹©è¦åŠ è½½çš„ç¯å¢ƒä¾èµ–
save_path: "./scores.json" # è¾“å‡ºåˆ†æ•°å­˜å‚¨è·¯å¾„
data:
  text:
    eval_data_path: "demos/text_eval/fineweb_5_samples.json" # å¾…è¯„ä¼°æ•°æ®è·¯å¾„
    ref_data_path: "demos/text_eval/alpaca_5_samples.json" # å‚è€ƒæ•°æ®è·¯å¾„
    ref_key: 'output' # å‚è€ƒæ•°æ®é”®å
    eval_key: 'text' # å¾…è¯„ä¼°æ•°æ®é”®å
    formatter: 'GenTextFormatter' # æ•°æ®åŠ è½½å™¨ç±»å‹ï¼Œä½¿ç”¨GenTextFormatterå³å¯

scorers:
  BleuScorer:
    n: 4 # Maximum value of N-gram
    eff: "average"  # Reference length selection method: "shortest", "average", "closest"
    special_reflen: null  # Set this value if a special reference sentence length is required

```

å…¨éƒ¨æ‰“åˆ†å™¨é…ç½®ä¿å­˜åœ¨`DataFlow/configs/eval/all_scorers.yaml`ä¸­ï¼Œç”Ÿæˆæ–‡æœ¬è¯„ä¼°æ‰“åˆ†å™¨é…ç½®ä¿å­˜åœ¨`DataFlow/configs/eval/gen_text_scorers.yaml`ä¸­ã€‚ä½¿ç”¨æ—¶å¯ä»¥ç›´æ¥å¤åˆ¶ç²˜è´´å…·ä½“æ‰“åˆ†å™¨é…ç½®ä¿¡æ¯ã€‚

## ğŸŒŸ æ•°æ®é›†ç¤ºä¾‹

æœ¬æ–‡æœ¬æ•°æ®è¯„ä¼°ç³»ç»ŸåŒæ—¶æ”¯æŒé¢„è®­ç»ƒæ•°æ®å’ŒSFTæ•°æ®æ ¼å¼ã€‚

### é¢„è®­ç»ƒæ•°æ®é›†ç¤ºä¾‹ï¼ˆæ‘˜è‡ª`Fineweb`ï¼‰ï¼š
```json
[
    {
        "text": "On Tuesday, NASCAR announced the release of \u201cNASCAR Classic Races, Volume 1,\u201d available on iTunes.",
        "id": "<urn:uuid:5189a256-bd76-489b-948e-9300a6f3f9da>"
    },
    {
        "text": "Tiger, GA Homeowners Insurance\nGet cheap home insurance in Tiger, GA within minutes. ",
        "id": "<urn:uuid:b49eaf47-48ed-4ff1-9121-f9e36247831f>"
    }
]
```
è‹¥è¦å¯¹ä¸Šè¿°æ•°æ®æ ¼å¼è¿›è¡Œè¯„ä¼°ï¼Œå¯æŒ‡å®š`keys: text`

### SFTæ•°æ®é›†ç¤ºä¾‹ï¼ˆæ‘˜è‡ª`alpaca-cleaned`ï¼‰
```json
[
    {
        "instruction": "Rearrange the following sentence to make the sentence more interesting.",
        "input": "She left the party early",
        "output": "Early, she left the party."
    },
    {
        "instruction": "Let \n f(x) = {[ -x - 3 if x \u2264 1,; x/2 + 1 if x > 1. ].\nFind the sum of all values of x such that f(x) = 0.",
        "input": "",
        "output": "We solve the equation f(x) = 0 on the domains x \u2264 1 and x > 1.\n\nIf x \u2264 1, then f(x) = -x - 3, so we want to solve -x - 3 = 0. The solution is x = -3, which satisfies x \u2264 1.\n\nIf x > 1, then f(x) = x/2 + 1, so we want to solve x/2 + 1 = 0. The solution is x = -2, but this value does not satisfy x > 1.\n\nTherefore, the only solution is x = -3."
    }
]
```
è‹¥è¦å¯¹ä¸Šè¿°æ•°æ®æ ¼å¼è¿›è¡Œè¯„ä¼°ï¼Œå¯æŒ‡å®š`keys: ['instruction','input','output']`

## ğŸ’ª è¿è¡Œæ‰“åˆ†å™¨
åªéœ€è¦ä¸€è¡Œä»£ç ä¾¿å¯è¿è¡Œï¼š
```bash
cd path/to/DataFlow
python eval.py --config configs/eval/text_scorer_example2.yaml
```
è¾“å‡ºæ‰“åˆ†é»˜è®¤è·¯å¾„åœ¨
```
./scores.json
```
ä¹Ÿå¯ä»¥è‡ªè¡Œåœ¨yamlçš„save_pathæŒ‡å®š

## ğŸ“Œ è¾“å‡ºç¤ºä¾‹
å…¶ä¸­ï¼Œ`meta_scores`ä¸­ä¿å­˜å¯¹æ•´ä¸ªæ•°æ®é›†å±‚é¢çš„æ‰“åˆ†å™¨å¾—åˆ†ï¼Œæ¯”å¦‚`VendiScore`ã€‚`item_scores`åˆ™ä¿å­˜æ•°æ®é›†ä¸­æ¯ä¸€æ¡æ•°æ®çš„å•ç‹¬å¾—åˆ†ã€‚
```json
{
    "meta_scores": {},
    "item_scores": {
        "0": {
            "QuratingScore": {
                "QuratingWritingStyleScore": -0.3477,
                "QuratingRequiredExpertiseScore": -0.9062,
                "QuratingFactsAndTriviaScore": 1.789,
                "QuratingEducationalValueScore": 0.02051
            },
            "PresidioScore": {
                "Default": 6.0
            }
        },
        "1": {
            "QuratingScore": {
                "QuratingWritingStyleScore": -1.584,
                "QuratingRequiredExpertiseScore": -2.233,
                "QuratingFactsAndTriviaScore": -2.279,
                "QuratingEducationalValueScore": 1.518
            },
            "PresidioScore": {
                "Default": 4.0
            }
        },
        "2": {
            "QuratingScore": {
                "QuratingWritingStyleScore": 2.433,
                "QuratingRequiredExpertiseScore": 1.782,
                "QuratingFactsAndTriviaScore": 0.7237,
                "QuratingEducationalValueScore": 7.503
            },
            "PresidioScore": {
                "Default": 71.0
            }
        },
        "3": {
            "QuratingScore": {
                "QuratingWritingStyleScore": -2.444,
                "QuratingRequiredExpertiseScore": -0.1224,
                "QuratingFactsAndTriviaScore": 1.851,
                "QuratingEducationalValueScore": 4.234
            },
            "PresidioScore": {
                "Default": 16.0
            }
        },
        "4": {
            "QuratingScore": {
                "QuratingWritingStyleScore": -1.711,
                "QuratingRequiredExpertiseScore": -6.969,
                "QuratingFactsAndTriviaScore": -4.281,
                "QuratingEducationalValueScore": -6.125
            },
            "PresidioScore": {
                "Default": 2.0
            }
        }
    }
}
```
